{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomNumbers = [-0.23, 0.08, 0.81, 0.71, 0.85, -0.19, -0.63, 0.94, 0.19, 0.87, 0.64, -0.83]\n",
    "\n",
    "bh = [0.06, 0.73, 0.34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8   1.75  1.34]\n",
      " [ 0.07  2.39  0.51]\n",
      " [ 1.64  2.22 -0.68]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "wh = np.array([[-0.23, 0.08, 0.81], [0.71, 0.85, -0.19],[-0.63, 0.94, 0.19], [0.87, 0.64, -0.83]])\n",
    "\n",
    "biases = np.array([0.06, 0.73, 0.34])\n",
    "hidden_layer_input = np.dot(x, wh)+ biases\n",
    "print(hidden_layer_input)\n",
    "#[[-0.8   1.75  1.34]\n",
    "# [ 0.07  2.39  0.51]\n",
    "# [ 1.64  2.22 -0.68]]\n",
    "\n",
    "#Note: Sigmoid for claulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09]\n",
      " [ 0.09]\n",
      " [-0.89]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return round(1 / (1 + np.exp(-x)),2)\n",
    "\n",
    "hidden_layer_input = np.array([[-0.8,   1.75,  1.34],[ 0.07,  2.39,  0.51],[ 1.64,  2.22, -0.68]])\n",
    "#hiddenlayer_activations =  relu(hidden_layer_input);\n",
    "hiddenlayer_activations = np.array([[0.31   ,  0.85 ,  0.79 ],[0.51   ,  0.91 ,  0.62 ],[0.83   ,  0.90 ,  0.33 ]])\n",
    "#[[0 ,  1.75,  1.34]\n",
    "#[ 0.07,  2.39,  0.51]\n",
    "#[ 1.64,  2.22, 0]]\n",
    "\n",
    "bout = np.array([0.69])\n",
    "wout = np.array([[0.36], [0.85], [0.98]])\n",
    "\n",
    "output_layer_input = np.dot(hiddenlayer_activations,wout ) + bout\n",
    "#[[ 2.2983],\n",
    "#[ 2.2547],\n",
    "#[ 2.0772]]\n",
    "\n",
    "# output = sigmoid(output_layer_input)\n",
    "output = np.array([[0.91],[0.91],[0.89]])\n",
    "y = np.array([[1],[1],[0]])\n",
    "Eout = y - output\n",
    "print(Eout)\n",
    "#[[ 0.09],\n",
    "#[ 0.09],\n",
    "#[-0.89]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsigmoid(y):\n",
    "    return round(y * (1.0 - y),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17]\n",
      " [ 0.17]\n",
      " [-0.79]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "slope_hidden_layer = np.array([[  0.21, 0.13, 0.17],[  0.25, 0.08, 0.24],[  0.14, 0.09, 0.22]])\n",
    "slope_output_layer = np.array([[0.08],[0.08],[0.10] ])\n",
    "d_output = np.add(Eout, slope_output_layer)\n",
    "print(d_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0612  0.1445  0.1666]\n",
      " [ 0.0612  0.1445  0.1666]\n",
      " [-0.2844 -0.6715 -0.7742]]\n",
      "[[ 0.012852  0.018785  0.028322]\n",
      " [ 0.0153    0.01156   0.039984]\n",
      " [-0.039816 -0.060435 -0.170324]]\n"
     ]
    }
   ],
   "source": [
    "Error_at_hidden_layer = np.dot(d_output, wout.T)\n",
    "print(Error_at_hidden_layer)\n",
    "#[[ 0.0612  0.1445  0.1666]\n",
    "# [ 0.0612  0.1445  0.1666]\n",
    "# [-0.2844 -0.6715 -0.7742]]\n",
    "\n",
    "d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "print(d_hiddenlayer)\n",
    "#[[ 0.012852  0.018785  0.028322]\n",
    "# [ 0.0153    0.01156   0.039984]\n",
    "# [-0.039816 -0.060435 -0.170324]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00141]\n",
      " [ 0.56174]\n",
      " [ 0.9653 ]]\n",
      "[[-0.2102936  0.1012415  0.8578142]\n",
      " [ 0.6821288  0.8076955 -0.3092268]\n",
      " [-0.6102936  0.9612415  0.2378142]\n",
      " [ 0.8528388  0.6057875 -0.921238 ]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "wout = wout + np.dot(hiddenlayer_activations.T, d_output) * learning_rate\n",
    "print(wout)\n",
    "#[-0.00141]\n",
    "# [ 0.56174]\n",
    "# [ 0.9653 ]]\n",
    "\n",
    "wh = wh + np.dot(x.T,d_hiddenlayer) * learning_rate\n",
    "print(wh)\n",
    "#[[-0.2102936  0.1012415  0.8578142]\n",
    "# [ 0.6821288  0.8076955 -0.3092268]\n",
    "# [-0.6102936  0.9612415  0.2378142]\n",
    "# [ 0.8528388  0.6057875 -0.921238 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13126924  0.83085168  0.51976231]]\n",
      "[ 0.645]\n"
     ]
    }
   ],
   "source": [
    "bh = bh + np.sum(d_hiddenlayer, axis=0) * learning_rate\n",
    "print(bh)\n",
    "#[[ 0.13126924  0.83085168  0.51976231]]\n",
    "\n",
    "bout = bout + np.sum(d_output, axis=0)*learning_rate\n",
    "print(bout)\n",
    "#[ 0.645]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82729681]\n",
      " [ 0.84430268]\n",
      " [ 0.82620679]]\n",
      "[[ 0.8248064 ]\n",
      " [ 0.84173301]\n",
      " [ 0.82326586]]\n",
      "[[ 0.82229778]\n",
      " [ 0.83913906]\n",
      " [ 0.82029603]]\n",
      "[[ 0.81977263]\n",
      " [ 0.83652247]\n",
      " [ 0.81729911]]\n",
      "[[ 0.81723273]\n",
      " [ 0.83388503]\n",
      " [ 0.814277  ]]\n",
      "[[ 0.81467992]\n",
      " [ 0.83122856]\n",
      " [ 0.81123169]]\n",
      "[[ 0.81211614]\n",
      " [ 0.82855503]\n",
      " [ 0.80816528]]\n",
      "[[ 0.80954337]\n",
      " [ 0.82586646]\n",
      " [ 0.80507996]]\n",
      "[[ 0.80696368]\n",
      " [ 0.82316497]\n",
      " [ 0.801978  ]]\n",
      "[[ 0.8043792 ]\n",
      " [ 0.82045274]\n",
      " [ 0.79886174]]\n",
      "[[ 0.80179209]\n",
      " [ 0.81773202]\n",
      " [ 0.7957336 ]]\n",
      "[[ 0.79920457]\n",
      " [ 0.81500513]\n",
      " [ 0.79259603]]\n",
      "[[ 0.7966189 ]\n",
      " [ 0.81227442]\n",
      " [ 0.78945155]]\n",
      "[[ 0.79403736]\n",
      " [ 0.8095423 ]\n",
      " [ 0.78630273]]\n",
      "[[ 0.79146223]\n",
      " [ 0.80681119]\n",
      " [ 0.78315213]]\n",
      "[[ 0.78889583]\n",
      " [ 0.80408354]\n",
      " [ 0.78000235]]\n",
      "[[ 0.78634045]\n",
      " [ 0.80136183]\n",
      " [ 0.776856  ]]\n",
      "[[ 0.7837984 ]\n",
      " [ 0.79864852]\n",
      " [ 0.77371568]]\n",
      "[[ 0.78127194]\n",
      " [ 0.79594605]\n",
      " [ 0.77058395]]\n",
      "[[ 0.77876332]\n",
      " [ 0.79325686]\n",
      " [ 0.76746337]]\n",
      "[[ 0.77627475]\n",
      " [ 0.79058336]\n",
      " [ 0.76435645]]\n",
      "[[ 0.77380839]\n",
      " [ 0.78792791]\n",
      " [ 0.76126565]]\n",
      "[[ 0.77136634]\n",
      " [ 0.78529282]\n",
      " [ 0.75819338]]\n",
      "[[ 0.76895065]\n",
      " [ 0.78268034]\n",
      " [ 0.75514197]]\n",
      "[[ 0.76656328]\n",
      " [ 0.78009267]\n",
      " [ 0.75211367]]\n",
      "[[ 0.76420614]\n",
      " [ 0.77753192]\n",
      " [ 0.74911065]]\n",
      "[[ 0.76188102]\n",
      " [ 0.7750001 ]\n",
      " [ 0.74613498]]\n",
      "[[ 0.75958966]\n",
      " [ 0.77249917]\n",
      " [ 0.74318865]]\n",
      "[[ 0.75733367]\n",
      " [ 0.77003096]\n",
      " [ 0.74027351]]\n",
      "[[ 0.75511458]\n",
      " [ 0.76759722]\n",
      " [ 0.73739134]]\n",
      "[[ 0.75293382]\n",
      " [ 0.76519957]\n",
      " [ 0.73454375]]\n",
      "[[ 0.75079271]\n",
      " [ 0.76283955]\n",
      " [ 0.73173229]]\n",
      "[[ 0.74869246]\n",
      " [ 0.76051857]\n",
      " [ 0.72895834]]\n",
      "[[ 0.74663417]\n",
      " [ 0.75823793]\n",
      " [ 0.72622319]]\n",
      "[[ 0.74461884]\n",
      " [ 0.75599882]\n",
      " [ 0.72352799]]\n",
      "[[ 0.74264736]\n",
      " [ 0.7538023 ]\n",
      " [ 0.72087377]]\n",
      "[[ 0.7407205 ]\n",
      " [ 0.75164933]\n",
      " [ 0.71826143]]\n",
      "[[ 0.73883894]\n",
      " [ 0.74954075]\n",
      " [ 0.71569177]]\n",
      "[[ 0.73700324]\n",
      " [ 0.74747727]\n",
      " [ 0.71316545]]\n",
      "[[ 0.73521387]\n",
      " [ 0.74545952]\n",
      " [ 0.710683  ]]\n",
      "[[ 0.73347118]\n",
      " [ 0.743488  ]\n",
      " [ 0.70824488]]\n",
      "[[ 0.73177544]\n",
      " [ 0.74156311]\n",
      " [ 0.7058514 ]]\n",
      "[[ 0.73012682]\n",
      " [ 0.73968514]\n",
      " [ 0.70350278]]\n",
      "[[ 0.72852541]\n",
      " [ 0.73785429]\n",
      " [ 0.70119913]]\n",
      "[[ 0.7269712 ]\n",
      " [ 0.73607066]\n",
      " [ 0.69894047]]\n",
      "[[ 0.72546411]\n",
      " [ 0.73433426]\n",
      " [ 0.69672673]]\n",
      "[[ 0.72400396]\n",
      " [ 0.73264503]\n",
      " [ 0.69455775]]\n",
      "[[ 0.72259054]\n",
      " [ 0.73100281]\n",
      " [ 0.69243327]]\n",
      "[[ 0.72122352]\n",
      " [ 0.72940736]\n",
      " [ 0.69035298]]\n",
      "[[ 0.71990254]\n",
      " [ 0.72785838]\n",
      " [ 0.68831648]]\n",
      "[[ 0.71862718]\n",
      " [ 0.72635552]\n",
      " [ 0.6863233 ]]\n",
      "[[ 0.71739696]\n",
      " [ 0.72489832]\n",
      " [ 0.68437294]]\n",
      "[[ 0.71621133]\n",
      " [ 0.72348632]\n",
      " [ 0.68246479]]\n",
      "[[ 0.71506972]\n",
      " [ 0.72211896]\n",
      " [ 0.68059824]]\n",
      "[[ 0.71397152]\n",
      " [ 0.72079566]\n",
      " [ 0.6787726 ]]\n",
      "[[ 0.71291607]\n",
      " [ 0.71951579]\n",
      " [ 0.67698715]]\n",
      "[[ 0.71190268]\n",
      " [ 0.71827868]\n",
      " [ 0.67524114]]\n",
      "[[ 0.71093062]\n",
      " [ 0.71708362]\n",
      " [ 0.67353377]]\n",
      "[[ 0.70999915]\n",
      " [ 0.71592989]\n",
      " [ 0.67186422]]\n",
      "[[ 0.70910752]\n",
      " [ 0.71481672]\n",
      " [ 0.67023165]]\n",
      "[[ 0.70825492]\n",
      " [ 0.71374333]\n",
      " [ 0.66863518]]\n",
      "[[ 0.70744056]\n",
      " [ 0.71270891]\n",
      " [ 0.66707394]]\n",
      "[[ 0.70666363]\n",
      " [ 0.71171265]\n",
      " [ 0.66554702]]\n",
      "[[ 0.70592331]\n",
      " [ 0.71075372]\n",
      " [ 0.66405351]]\n",
      "[[ 0.70521876]\n",
      " [ 0.70983127]\n",
      " [ 0.6625925 ]]\n",
      "[[ 0.70454915]\n",
      " [ 0.70894446]\n",
      " [ 0.66116306]]\n",
      "[[ 0.70391364]\n",
      " [ 0.70809242]\n",
      " [ 0.65976427]]\n",
      "[[ 0.70331139]\n",
      " [ 0.70727431]\n",
      " [ 0.6583952 ]]\n",
      "[[ 0.70274158]\n",
      " [ 0.70648928]\n",
      " [ 0.65705491]]\n",
      "[[ 0.70220337]\n",
      " [ 0.70573645]\n",
      " [ 0.6557425 ]]\n",
      "[[ 0.70169594]\n",
      " [ 0.705015  ]\n",
      " [ 0.65445704]]\n",
      "[[ 0.70121846]\n",
      " [ 0.70432406]\n",
      " [ 0.65319762]]\n",
      "[[ 0.70077012]\n",
      " [ 0.70366281]\n",
      " [ 0.65196334]]\n",
      "[[ 0.70035013]\n",
      " [ 0.70303042]\n",
      " [ 0.65075332]]\n",
      "[[ 0.6999577 ]\n",
      " [ 0.70242606]\n",
      " [ 0.64956667]]\n",
      "[[ 0.69959204]\n",
      " [ 0.70184893]\n",
      " [ 0.64840252]]\n",
      "[[ 0.69925239]\n",
      " [ 0.70129823]\n",
      " [ 0.64726002]]\n",
      "[[ 0.69893799]\n",
      " [ 0.70077318]\n",
      " [ 0.64613832]]\n",
      "[[ 0.69864811]\n",
      " [ 0.70027301]\n",
      " [ 0.64503661]]\n",
      "[[ 0.69838201]\n",
      " [ 0.69979695]\n",
      " [ 0.64395408]]\n",
      "[[ 0.698139  ]\n",
      " [ 0.69934428]\n",
      " [ 0.64288991]]\n",
      "[[ 0.69791838]\n",
      " [ 0.69891426]\n",
      " [ 0.64184335]]\n",
      "[[ 0.69771946]\n",
      " [ 0.69850619]\n",
      " [ 0.64081363]]\n",
      "[[ 0.69754159]\n",
      " [ 0.69811936]\n",
      " [ 0.6398    ]]\n",
      "[[ 0.69738411]\n",
      " [ 0.69775311]\n",
      " [ 0.63880173]]\n",
      "[[ 0.69724639]\n",
      " [ 0.69740677]\n",
      " [ 0.63781811]]\n",
      "[[ 0.69712783]\n",
      " [ 0.69707969]\n",
      " [ 0.63684846]]\n",
      "[[ 0.69702782]\n",
      " [ 0.69677124]\n",
      " [ 0.63589209]]\n",
      "[[ 0.69694578]\n",
      " [ 0.69648083]\n",
      " [ 0.63494835]]\n",
      "[[ 0.69688114]\n",
      " [ 0.69620784]\n",
      " [ 0.6340166 ]]\n",
      "[[ 0.69683336]\n",
      " [ 0.6959517 ]\n",
      " [ 0.63309621]]\n",
      "[[ 0.6968019 ]\n",
      " [ 0.69571185]\n",
      " [ 0.63218657]]\n",
      "[[ 0.69678624]\n",
      " [ 0.69548775]\n",
      " [ 0.6312871 ]]\n",
      "[[ 0.69678589]\n",
      " [ 0.69527886]\n",
      " [ 0.63039723]]\n",
      "[[ 0.69680035]\n",
      " [ 0.69508467]\n",
      " [ 0.62951639]]\n",
      "[[ 0.69682915]\n",
      " [ 0.69490469]\n",
      " [ 0.62864405]]\n",
      "[[ 0.69687185]\n",
      " [ 0.69473843]\n",
      " [ 0.62777968]]\n",
      "[[ 0.69692799]\n",
      " [ 0.69458542]\n",
      " [ 0.62692278]]\n",
      "[[ 0.69699715]\n",
      " [ 0.69444522]\n",
      " [ 0.62607285]]\n",
      "[[ 0.69707891]\n",
      " [ 0.69431739]\n",
      " [ 0.62522942]]\n"
     ]
    }
   ],
   "source": [
    "# Another example \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y=np.array([[1],[1],[0]])\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "\n",
    "#Variable initialization\n",
    "epoch=100 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "    #Forward Propogation\n",
    "    hidden_layer_input1=np.dot(X,wh)\n",
    "    hidden_layer_input=hidden_layer_input1 + bh\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "    output_layer_input= output_layer_input1+ bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    #Backpropagation\n",
    "    E = y-output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    d_output = E * slope_output_layer\n",
    "    Error_at_hidden_layer = d_output.dot(wout.T)\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += hiddenlayer_activations.T.dot(d_output) *lr\n",
    "    bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "    wh += X.T.dot(d_hiddenlayer) *lr\n",
    "    bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
    "\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}